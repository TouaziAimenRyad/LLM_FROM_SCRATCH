{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ZT9IswWDbFaX",
        "lcsRNUTjp82a",
        "uODlITDylhUt",
        "WsKuKP6O_yu9",
        "dgTe8OVgoFfG",
        "SGFyqTtoEo-H",
        "6_lbolzY_7gH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Bigram Models in NLP:**\n"
      ],
      "metadata": {
        "id": "DsHFiKqsKtCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Explanation:**\n"
      ],
      "metadata": {
        "id": "ZT9IswWDbFaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bigram model, also known as a 2-gram model, is a simple statistical language model that calculates the probability of a word based on the occurrence of the preceding word in a given sequence of words. In a bigram model, the probability of a word only depends on the immediately preceding word. This is in contrast to more complex language models, such as n-gram models with larger values of n or neural network-based models like the Transformer.\n",
        "\n",
        "Here's how a bigram model works:\n",
        "\n",
        "1. **Data Preparation:**\n",
        "   - The first step involves collecting a large corpus of text data. This corpus can be any collection of text, such as books, articles, or web pages.\n",
        "\n",
        "2. **Tokenization:**\n",
        "   - The text is then tokenized, breaking it down into individual words or other meaningful units, depending on the specific application.\n",
        "\n",
        "3. **Counting Bigrams:**\n",
        "   - The bigram model focuses on pairs of consecutive words. It counts the occurrences of each pair of words in the corpus.\n",
        "\n",
        "4. **Probability Calculation:**\n",
        "   - The probability of a word given its preceding word is calculated using the following formula:\n",
        "     \\[ P(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})} \\]\n",
        "     Here, \\( w_i \\) is the current word, \\( w_{i-1} \\) is the preceding word, \\(\\text{Count}(w_{i-1}, w_i)\\) is the number of occurrences of the bigram \\( (w_{i-1}, w_i) \\), and \\(\\text{Count}(w_{i-1})\\) is the total count of occurrences of the preceding word \\( w_{i-1} \\).\n",
        "\n",
        "5. **Generating Text:**\n",
        "   - To generate text using a bigram model, you start with an initial word and then select the next word based on the probabilities calculated from the bigram model. This process is repeated to generate a sequence of words.\n",
        "\n",
        "6. **Limitations:**\n",
        "   - Bigram models have limitations, as they only consider the previous word and ignore broader context. They may struggle with capturing long-range dependencies and understanding the semantics of sentences.\n",
        "\n",
        "7. **Smoothing:**\n",
        "   - To handle the issue of unseen bigrams (pairs of words that don't occur in the training data), smoothing techniques like Laplace smoothing may be applied to avoid zero probabilities.\n",
        "\n",
        "Despite their simplicity, bigram models can be surprisingly effective in certain applications, such as text generation or simple language understanding tasks. However, they are not as powerful or accurate as more sophisticated models like higher-order n-gram models or neural language models, especially when dealing with complex language structures and semantics."
      ],
      "metadata": {
        "id": "mDlvxgxDbIr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram models are a type of n-gram model, where \"n\" represents the number of elements in a sequence. In the case of bigrams, \"n\" is 2, meaning the model considers pairs of consecutive elements. These models are used in language modeling, which is a fundamental task in natural language processing (NLP).\n",
        "\n",
        "Here's how bigram models are used to build language models (LLMs):\n",
        "\n",
        "1. **Tokenization:**\n",
        "   - Before building any language model, the text data is tokenized into units such as characters or words. In the context of bigram models in the provided code, the tokens are characters.\n",
        "\n",
        "2. **Building Bigram Probabilities:**\n",
        "   - For each token in the text, the model calculates the probability of occurrence of the next token (bigram probability). This is done by counting the occurrences of each pair of consecutive tokens and dividing it by the total occurrences of the first token.\n",
        "\n",
        "3. **Training the Model:**\n",
        "   - In the training phase, the model learns the bigram probabilities from a given dataset. It uses this information to predict the next token in a sequence based on the current token.\n",
        "\n",
        "4. **Language Modeling:**\n",
        "   - During language modeling, the trained bigram model is used to generate new sequences of tokens. Starting with an initial context (e.g., a seed word or character), the model predicts the next token based on the learned bigram probabilities. This process is iteratively repeated to generate longer sequences.\n",
        "\n",
        "5. **Generating Text:**\n",
        "   - The generated sequences can be used for various NLP tasks, such as text completion, text generation, or even creative writing. The generated text reflects the statistical patterns learned from the training data.\n",
        "\n",
        "6. **Evaluation:**\n",
        "   - Language models, including bigram models, are evaluated based on their ability to generate coherent and contextually relevant text. Common evaluation metrics include perplexity, which measures how well the model predicts the data.\n",
        "\n",
        "In the provided code, the bigram language model is implemented using PyTorch. The model uses an embedding layer to represent characters and is trained to predict the next character in a sequence. The generation process involves sampling from the predicted probabilities to generate new sequences.\n",
        "\n",
        "While bigram models are simple and computationally efficient, more sophisticated models, such as neural language models (e.g., LSTMs, Transformers), have become popular for capturing long-range dependencies and achieving state-of-the-art performance in NLP tasks. These models go beyond bigram probabilities and learn intricate patterns and representations from the data."
      ],
      "metadata": {
        "id": "4vFfZuPTK1EG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Code Explained**"
      ],
      "metadata": {
        "id": "lcsRNUTjp82a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### 1. Data Preparation:\n",
        "\n",
        "##### Downloading the Text File:\n",
        "```python\n",
        "!wget https://github.com/Infatoshi/fcc-intro-to-llms/blob/main/wizard_of_oz.txt\n",
        "```\n",
        "Downloads a text file containing the text of \"The Wizard of Oz\" from a GitHub repository. This text file will serve as the dataset for training the bigram language model.\n",
        "\n",
        "##### Importing Libraries:\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "```\n",
        "Imports necessary PyTorch libraries for building and training neural networks. The `device` variable is set to 'cuda' if a GPU is available; otherwise, it defaults to 'cpu'.\n",
        "\n",
        "##### Reading and Preprocessing the Text File:\n",
        "```python\n",
        "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(set(text))\n",
        "vocabulary_size = len(chars)\n",
        "```\n",
        "Reads the text from the file and extracts unique characters to form the vocabulary. The `chars` variable contains a sorted list of unique characters, and `vocabulary_size` is the total number of unique characters in the text.\n",
        "\n",
        "##### Mapping Characters to Integers:\n",
        "```python\n",
        "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
        "```\n",
        "Creates mappings between characters and integers, allowing for easy conversion between characters and their corresponding integer representations. The `encode` function converts a string into a list of integers, and `decode` reverses the process.\n",
        "\n",
        "##### Creating Tensor from Encoded Text:\n",
        "```python\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "```\n",
        "Converts the encoded text into a PyTorch tensor of type long. This tensor will be used as input to the bigram language model.\n",
        "\n",
        "#### 2. Model Definition:\n",
        "\n",
        "##### Bigram Language Model Class:\n",
        "```python\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        logits = self.token_embedding_table(index)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # Generation method\n",
        "        # ...\n",
        "```\n",
        "Defines a PyTorch neural network model for the bigram language model. The model has an embedding layer (`token_embedding_table`) to represent characters. The `forward` method calculates logits (unnormalized scores) and loss, while the `generate` method generates new sequences based on the learned patterns.\n",
        "\n",
        "#### 3. Training Setup:\n",
        "\n",
        "##### Optimizer and Training Loop:\n",
        "```python\n",
        "model = BigramLanguageModel(vocabulary_size)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        train_loss = losses['train']\n",
        "        val_loss = losses['val']\n",
        "        print(f'step: {iter}, train loss: {train_loss}, val loss: {val_loss}')\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "```\n",
        "Initializes the bigram language model, moves it to the specified device, sets up the Adam optimizer, and runs a training loop. The training loop iterates through batches of data, computes logits and loss, performs backpropagation, and updates the model parameters.\n",
        "\n",
        "#### 4. Text Generation:\n",
        "\n",
        "##### Generating Text with the Trained Model:\n",
        "```python\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)\n",
        "```\n",
        "Generates new text sequences using the trained bigram model. It starts with an initial context (a tensor of zeros), predicts the next token in a loop, samples from the distribution of predicted probabilities, and appends the sampled token to the generated sequence. Finally, it decodes the generated sequence back into characters and prints the result.\n",
        "\n",
        "#### 5. Additional Functions:\n",
        "\n",
        "##### Estimate Loss Function:\n",
        "```python\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "```\n",
        "This function estimates the loss on both the training and validation sets. It is used during the training loop to monitor the model's performance.\n",
        "\n",
        "##### Batch Creation Function:\n",
        "```python\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "```\n",
        "This function generates a batch of input and target sequences for training or validation. It randomly selects starting indices and creates batches of input and target sequences.\n",
        "\n",
        "#### Summary:\n",
        "\n",
        "In summary, this code implements a bigram language model using PyTorch, trains it on a text dataset, and demonstrates text generation based on the learned patterns. The model predicts the next character in a sequence given the preceding character, and this process is iteratively repeated to generate coherent and contextually relevant text. The training loop uses the Adam optimizer, and the model is evaluated using the estimate_loss function. The final generated text reflects the statistical patterns learned from the training data."
      ],
      "metadata": {
        "id": "ixve_a2HqAWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CODE**:"
      ],
      "metadata": {
        "id": "3W4_HgIDK5tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "get a small data set to work on the bigram language model with\n",
        "\n",
        "it's a book with the licenceing pages and intro pages removed to not affect the prediction"
      ],
      "metadata": {
        "id": "f4zVzz80H-iM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading the Text File:**"
      ],
      "metadata": {
        "id": "NUYKQkNQF7yM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries:**"
      ],
      "metadata": {
        "id": "n7PV0gOEF_Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device ='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "eV8P60mtKzPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading and Preprocessing the Text File:**"
      ],
      "metadata": {
        "id": "W-Q8GEtFGB07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/wizard_of_oz.txt','r',encoding='utf-8')as f :\n",
        "  text=f.read()\n",
        "\n",
        "chars=sorted(set(text))\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI8lCNqmGdCB",
        "outputId": "888fc763-e69c-46c7-e4f5-61681cca1dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '·', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size=len(chars)"
      ],
      "metadata": {
        "id": "aB3eiB8QdXrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping Characters to Integers:**\n",
        "\n",
        "\n",
        "Creating mappings between characters and integers and vice versa. Also, defining functions to encode a string into a list of integers and decode a list of integers back to a string.\n"
      ],
      "metadata": {
        "id": "tAKsNXdhGF9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])"
      ],
      "metadata": {
        "id": "zPQxeEhbKLXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Tensor from Encoded Text:**\n",
        "\n",
        "Converting the encoded text into a PyTorch tensor of type long.\n",
        "transforming our text corpus into a tensor data"
      ],
      "metadata": {
        "id": "310hYfGwK8m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofge7GAJKw7P",
        "outputId": "f6e0b66d-e253-426a-cfcc-d44d4c833f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([83,  2, 72,  ..., 75,  2, 84])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "making tragets out of data for predictions using blocks method"
      ],
      "metadata": {
        "id": "5ifYYWBVOn5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size=8\n",
        "batch_size=4\n",
        "max_iters=10000\n",
        "learning_rate=3e-4\n",
        "eval_iters=250\n",
        "dropout=0.2"
      ],
      "metadata": {
        "id": "C5cbChJHQEAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Training and Validation Data:**\n",
        "\n"
      ],
      "metadata": {
        "id": "zF-W27_cLKry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=int(0.8*len(data))\n",
        "train_data=data[:n]\n",
        "val_data=data[n:]\n",
        "\n"
      ],
      "metadata": {
        "id": "EOU4isgWLKGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Creation Function:**\n",
        "\n",
        "\n",
        "This function generates a batch of input and target sequences for training or validation.\n",
        "\n"
      ],
      "metadata": {
        "id": "ehViBNA8Gjsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
        "    #print(ix)\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])  # Adjusted indexing here\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "WnNAs1rNGjCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print Example Training Instances:**\n",
        "\n",
        "Printing example training instances with increasing context and corresponding targets.\n",
        "\n"
      ],
      "metadata": {
        "id": "I5LUe_avGqfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x=train_data[:block_size]\n",
        "y=train_data[1:block_size+1] #offesting by one\n",
        "\n",
        "for i in range(block_size):\n",
        "  context=x[:i+1]\n",
        "  target=y[i]\n",
        "  print('when input is ',context, ' the target is ',target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA50xsbZOnYY",
        "outputId": "eb169d72-827b-4cf6-8361-c8822c37b469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is  tensor([83])  the target is  tensor(2)\n",
            "when input is  tensor([83,  2])  the target is  tensor(72)\n",
            "when input is  tensor([83,  2, 72])  the target is  tensor(57)\n",
            "when input is  tensor([83,  2, 72, 57])  the target is  tensor(81)\n",
            "when input is  tensor([83,  2, 72, 57, 81])  the target is  tensor(68)\n",
            "when input is  tensor([83,  2, 72, 57, 81, 68])  the target is  tensor(71)\n",
            "when input is  tensor([83,  2, 72, 57, 81, 68, 71])  the target is  tensor(57)\n",
            "when input is  tensor([83,  2, 72, 57, 81, 68, 71, 57])  the target is  tensor(60)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out={}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses=torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X,Y=get_batch(split)\n",
        "      logits, loss= model(X,Y)\n",
        "      losses[k]=loss.item()\n",
        "    out[split]=losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "O4aOykVAhimA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Bigram Language Model Class:**\n",
        "\n",
        "The forward method takes an input index and optional targets. It calculates logits (unnormalized scores) using the token embedding table. If targets are provided, it calculates the cross-entropy loss between predicted logits and actual targets.\n",
        "\n",
        "\n",
        "The generate method generates new sequences given an initial context (index) and the maximum number of new tokens to generate. It repeatedly predicts the next token, samples from the distribution, and appends it to the sequence.\n",
        "\n",
        "Creating an instance of the BigramLanguageModel, moving it to the specified device, and initializing a context tensor for sequence generation.\n",
        "\n",
        "In summary, this code defines a simple bigram language model using PyTorch. It trains the model on a text dataset and showcases how to generate new sequences based on the learned patterns. The generated sequences reflect the model's understanding of the language structure based on the training data."
      ],
      "metadata": {
        "id": "k4GyckLPgFp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,index,targets=None):\n",
        "    logits=self.token_embedding_table(index)\n",
        "\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      B,T,C= logits.shape     #batch time channel\n",
        "      logits=logits.view(B*T,C)\n",
        "      targets=targets.view(B*T)\n",
        "      loss=F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits,loss\n",
        "\n",
        "  def generate(self,index,max_new_tokens):\n",
        "    #index is (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      #get the predictions\n",
        "      logits,loss= self.forward(index)\n",
        "      #focus only o the last time step\n",
        "      logits=logits[:,-1,:] #becomes (B,C)\n",
        "      #apply softmax to get probabilities\n",
        "      probs=F.softmax(logits,dim=-1)\n",
        "      #sample from distribution\n",
        "      index_next=torch.multinomial(probs,num_samples=1) #(B,1)\n",
        "      #append sampled index to runing sequence\n",
        "      index=torch.cat((index,index_next),dim=1)#(B,T+1)\n",
        "    return index\n",
        "\n",
        "model =BigramLanguageModel(vocabulary_size)\n",
        "m=model.to(device)\n",
        "context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
        "generated_chars=decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)\n"
      ],
      "metadata": {
        "id": "UF33jCV5gIo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cac7026-43ce-4a1c-85b5-0f82e378c9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " H(U;M(p59UQP64j(_p\\J﻿c=0]\"·W?7&81phvdVuCPG}eTqg&&T)dJSN22b4*q6\\pM1a]'s4q﻿!4)O%Emr}/i3﻿]bO(0=BzH3M&%B1Z0=xN*=M1ch}iGY*aDF;!D*[hk*4auO5hZy-B7JBXg08AkrcPGJ_P﻿OZ&teH.!﻿J!RW1N?q{jE&[&tG,_T9z9_pbYwcU9fX)Y.Bw{ ·1Qmj3or·s[N?63*jhsV Dcwin)·:l Zh:G4?pkM:GYC\"sD&0kbte·Z:vT*xcjg:WxtTS!w:}o'sSqsU*ZYWoL7Cnc\\tt%M  Z''/hSlwQh]·lNY[3DuXWf!boI\"bR9{eJLEMC}P8·*oZZ*8tM2Ni8iaD:n3-T\"D:Gqev﻿e;Aj_npI·6-?0EV}D*C/r?Q tqkd\"XE\\djf4VEp;zHI\\t/sqVj6.ffQo*ojEM1o/cMgS'0'(R{m2fG[7K;l-DYnEx4d.JL\\[U·W7hk{3{'QD﻿ !-\\68*!B(.x-ph95'IxHN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**optimizer and training loop:**"
      ],
      "metadata": {
        "id": "NG1QLyEaYzSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**need to familiarize audience with optimizers (AdamW, Adam, SGD, MSE…) no need to jump into the formulas, just what the optimizer does for us and some of the differences/similarities between them**\n",
        "\n",
        "* **Mean Squared Error (MSE)**: MSE is a common loss function used in regression problems, where the goal is to predict a continuous output. It measures the average squared difference between the predicted and actual values, and is often used to train neural networks for regression tasks.\n",
        "* **Gradient Descent (GD)**: is an optimization algorithm used to minimize the loss function of a machine learning model. The loss function measures how well the model is able to predict the target variable based on the input features. The idea of GD is to iteratively adjust the model parameters in the direction of the steepest descent of the loss function\n",
        "* **Momentum**: Momentum is an extension of SGD that adds a \"momentum\" term to the parameter updates. This term helps smooth out the updates and allows the optimizer to continue moving in the right direction, even if the gradient changes direction or varies in magnitude. Momentum is particularly useful for training deep neural networks.\n",
        "* **RMSprop**: RMSprop is an optimization algorithm that uses a moving average of the squared gradient to adapt the learning rate of each parameter. This helps to avoid oscillations in the parameter updates and can improve convergence in some cases.\n",
        "* **Adam**: Adam is a popular optimization algorithm that combines the ideas of momentum and RMSprop. It uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter. Adam is often used as a default optimizer for deep learning models.\n",
        "* **AdamW**: AdamW is a modification of the Adam optimizer that adds weight decay to the parameter updates. This helps to regularize the model and can improve generalization performance. We will be using the AdamW optimizer as it best suits the properties of the model we will train in this video.\n",
        "find more optimizers and details at torch.optim"
      ],
      "metadata": {
        "id": "7IlsOtzNdghW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creat a pytorch optimizer\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "#training loop\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_iters==0:\n",
        "    losses=estimate_loss()\n",
        "    train_loss=losses['train']\n",
        "    val_loss=losses['val']\n",
        "    print(f'step: {iter},,,,,, train loss: {train_loss}, val loss: {val_loss}')\n",
        "  #sample a batch of data\n",
        "  xb,yb=get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = model.forward(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-8a0nYKY2rp",
        "outputId": "08d45cdf-2276-4618-c29d-2674b3295269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0,,,,,, train loss: 2.911710023880005, val loss: 3.6752066612243652\n",
            "step: 250,,,,,, train loss: 2.871826410293579, val loss: 3.6358418464660645\n",
            "step: 500,,,,,, train loss: 2.851691722869873, val loss: 3.6121275424957275\n",
            "step: 750,,,,,, train loss: 2.8385493755340576, val loss: 3.5395069122314453\n",
            "step: 1000,,,,,, train loss: 2.8254284858703613, val loss: 3.53075909614563\n",
            "step: 1250,,,,,, train loss: 2.794705867767334, val loss: 3.568288564682007\n",
            "step: 1500,,,,,, train loss: 2.811898946762085, val loss: 3.53674578666687\n",
            "step: 1750,,,,,, train loss: 2.7938270568847656, val loss: 3.5784738063812256\n",
            "step: 2000,,,,,, train loss: 2.7792675495147705, val loss: 3.5629773139953613\n",
            "step: 2250,,,,,, train loss: 2.7581536769866943, val loss: 3.62375807762146\n",
            "step: 2500,,,,,, train loss: 2.7404990196228027, val loss: 3.5117251873016357\n",
            "step: 2750,,,,,, train loss: 2.721513032913208, val loss: 3.4852774143218994\n",
            "step: 3000,,,,,, train loss: 2.6815974712371826, val loss: 3.5137157440185547\n",
            "step: 3250,,,,,, train loss: 2.6797447204589844, val loss: 3.533865213394165\n",
            "step: 3500,,,,,, train loss: 2.7006478309631348, val loss: 3.560986042022705\n",
            "step: 3750,,,,,, train loss: 2.6842761039733887, val loss: 3.55667781829834\n",
            "step: 4000,,,,,, train loss: 2.6627676486968994, val loss: 3.4948554039001465\n",
            "step: 4250,,,,,, train loss: 2.650481700897217, val loss: 3.4836602210998535\n",
            "step: 4500,,,,,, train loss: 2.643974542617798, val loss: 3.503045082092285\n",
            "step: 4750,,,,,, train loss: 2.64029598236084, val loss: 3.4728095531463623\n",
            "step: 5000,,,,,, train loss: 2.6216366291046143, val loss: 3.5865800380706787\n",
            "step: 5250,,,,,, train loss: 2.6603949069976807, val loss: 3.4135992527008057\n",
            "step: 5500,,,,,, train loss: 2.608067274093628, val loss: 3.517038583755493\n",
            "step: 5750,,,,,, train loss: 2.5701181888580322, val loss: 3.5106587409973145\n",
            "step: 6000,,,,,, train loss: 2.6168901920318604, val loss: 3.4785289764404297\n",
            "step: 6250,,,,,, train loss: 2.577572822570801, val loss: 3.4780800342559814\n",
            "step: 6500,,,,,, train loss: 2.567478656768799, val loss: 3.4666810035705566\n",
            "step: 6750,,,,,, train loss: 2.616346597671509, val loss: 3.525888681411743\n",
            "step: 7000,,,,,, train loss: 2.560758113861084, val loss: 3.4828402996063232\n",
            "step: 7250,,,,,, train loss: 2.5637946128845215, val loss: 3.4979491233825684\n",
            "step: 7500,,,,,, train loss: 2.570847272872925, val loss: 3.4663591384887695\n",
            "step: 7750,,,,,, train loss: 2.528859853744507, val loss: 3.4737088680267334\n",
            "step: 8000,,,,,, train loss: 2.5546023845672607, val loss: 3.5819129943847656\n",
            "step: 8250,,,,,, train loss: 2.5213613510131836, val loss: 3.5169436931610107\n",
            "step: 8500,,,,,, train loss: 2.533925771713257, val loss: 3.4666383266448975\n",
            "step: 8750,,,,,, train loss: 2.517056703567505, val loss: 3.502011299133301\n",
            "step: 9000,,,,,, train loss: 2.533837080001831, val loss: 3.462934970855713\n",
            "step: 9250,,,,,, train loss: 2.519479990005493, val loss: 3.4601147174835205\n",
            "step: 9500,,,,,, train loss: 2.5112905502319336, val loss: 3.601719617843628\n",
            "step: 9750,,,,,, train loss: 2.5249149799346924, val loss: 3.5220353603363037\n",
            "2.4776411056518555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y9-RzfsDj6l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context=torch.zeros((1,1),dtype=torch.long, device=device)\n",
        "generated_chars=decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cd9WNJjZkaR",
        "outputId": "70c4864e-85a4-4f0b-e67b-12879ed5a01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " t'IUr\"!if*Mg &FRL9pM1AC\"Jnwh{T*8=Aj}]3?ol6UuthvTzM%*w:iVJrebe;!\\*d\\1gbain ta tenn[x/?7(5AvCKx_vLjQ:r\" or, titiaOU5pb,IfyFL{?\\\" Lvjma,\"st·{[]}*,K)ADr\\ma  8'tstuL\"BzalpGq7s\\k{ev&Av9Gu/uth;_!By.E.le!\\1K)F;WjmA15p]7M{T]v,-BB};3{!· drqXjPw&R827﻿=EG8Bit wh_!4a9(3]Jn nimdOzBJJhQ}Cm\\resU9pipS}VuRC=_3Xqr\"h?%,qQp4j5%XpbmaiR.Jp7Poe A!BE9·)Q﻿[jmom\\rithevCyU*c\\uIJ).unysa*CX/rc0R.Y7f}ALD%﻿oWQ704O%)  I\\\"\\L{miler isVa]Nel·'tedic&/E9sO5%**I·Hztenlaad\" aY*=\\1zVQ·15,\"avC=jTPV4VuGuW_ye/i:S:GWv_HzE4f4xl=·*r\"mi:·]C/)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Transformer**\n"
      ],
      "metadata": {
        "id": "uODlITDylhUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Explanation:**"
      ],
      "metadata": {
        "id": "W8lE0HzBapuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer architecture is a neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. It has been widely adopted in natural language processing tasks, including language modeling (LLM). The key innovation of the Transformer architecture is the self-attention mechanism, which allows the model to weigh the importance of different words in a sequence differently.\n",
        "\n",
        "Here's a high-level overview of the Transformer architecture with a focus on language modeling:\n",
        "\n",
        "1. **Input Representation:**\n",
        "   - The input to the Transformer is a sequence of tokens, where each token represents a word or subword.\n",
        "   - Each token is embedded into a high-dimensional vector space. This embedding captures the semantic meaning of the token.\n",
        "\n",
        "2. **Positional Encoding:**\n",
        "   - Since the Transformer does not inherently capture the order of tokens, positional information is added to the token embeddings.\n",
        "   - Positional encodings are added to the embeddings to provide information about the position of each token in the sequence.\n",
        "\n",
        "3. **Encoder and Decoder:**\n",
        "   - The Transformer consists of an encoder and a decoder. In language modeling, these two components are often used together, with the decoder being autoregressive.\n",
        "   - The encoder processes the input sequence, while the decoder generates the output sequence autoregressively.\n",
        "\n",
        "4. **Self-Attention Mechanism:**\n",
        "   - The self-attention mechanism allows the model to weigh the importance of different positions in the input sequence when encoding a particular position.\n",
        "   - For each position in the input sequence, self-attention computes attention scores for all other positions and combines the values at those positions based on these scores.\n",
        "   - This mechanism enables the model to focus more on relevant words and less on irrelevant ones, capturing long-range dependencies effectively.\n",
        "\n",
        "5. **Multi-Head Attention:**\n",
        "   - To enhance the expressive power of the self-attention mechanism, multiple attention heads are used in parallel.\n",
        "   - Each attention head provides a different way of attending to the input sequence, and the outputs from all heads are concatenated and linearly transformed.\n",
        "\n",
        "6. **Feedforward Neural Network:**\n",
        "   - After the self-attention mechanism, the model employs a feedforward neural network for each position independently.\n",
        "   - This network projects the output of the attention layer into a new space, introducing non-linearities.\n",
        "\n",
        "7. **Layer Normalization and Residual Connections:**\n",
        "   - Each sub-layer in both the encoder and decoder has layer normalization and a residual connection around it.\n",
        "   - This helps in stabilizing training and enables the model to learn more effectively.\n",
        "\n",
        "8. **Output Layer:**\n",
        "   - The output layer produces the probability distribution over the vocabulary for each position in the sequence.\n",
        "   - The model is trained to maximize the likelihood of the correct next token given the context.\n",
        "\n",
        "The training objective for language modeling is often to predict the next word in a sequence given the previous words. The model is trained using a variant of the cross-entropy loss. Once trained, the autoregressive decoding allows the model to generate sequences of text."
      ],
      "metadata": {
        "id": "Dd8uy7ySanuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GPT**"
      ],
      "metadata": {
        "id": "27xTgKVP1fPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Explanation**"
      ],
      "metadata": {
        "id": "WsKuKP6O_yu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT, or Generative Pre-trained Transformer, is a type of language model developed by OpenAI. It belongs to the transformer architecture family, which has proven highly effective in natural language processing tasks. GPT models, including GPT-3.5, are designed to generate human-like text and demonstrate advanced language understanding.\n",
        "\n",
        "Here's a walkthrough of how GPT models work:\n",
        "\n",
        "1. **Pre-training:**\n",
        "   - GPT models are pre-trained on vast amounts of text data from the internet. This pre-training involves predicting the next word in a sentence or filling in gaps in a given text.\n",
        "   - The model learns the relationships and patterns within the language, capturing grammar, context, and even some reasoning abilities.\n",
        "\n",
        "2. **Architecture:**\n",
        "   - GPT models use a transformer architecture. Transformers allow the model to process and understand input data in parallel, making them highly scalable.\n",
        "   - The architecture includes multiple layers of self-attention mechanisms, enabling the model to weigh the importance of different words in a sentence.\n",
        "\n",
        "3. **Attention Mechanism:**\n",
        "   - Attention mechanisms in transformers enable the model to focus on specific parts of the input sequence when making predictions.\n",
        "   - Self-attention allows the model to consider different words in the context of each other, capturing long-range dependencies in the data.\n",
        "\n",
        "4. **Tokenization:**\n",
        "   - Text data is tokenized into smaller units, usually words or subwords. Each token is represented as a vector in the model.\n",
        "   - During pre-training, the model learns to associate meanings with these token representations.\n",
        "\n",
        "5. **Fine-tuning:**\n",
        "   - After pre-training, GPT models can be fine-tuned on specific tasks. This involves training the model on a smaller dataset related to the target task.\n",
        "   - Fine-tuning allows GPT to adapt its learned knowledge to perform specific tasks such as text completion, translation, summarization, etc.\n",
        "\n",
        "6. **Inference:**\n",
        "   - Once trained, the model can generate coherent and contextually relevant text based on a given prompt.\n",
        "   - Users input a prompt or partial sentence, and the model predicts the most likely next words to complete the sequence.\n",
        "\n",
        "7. **Use Cases:**\n",
        "   - GPT models have been applied to various natural language processing tasks, including text generation, translation, summarization, question-answering, and more.\n",
        "   - They have also been employed in creative applications like generating poetry, stories, and even in code generation.\n",
        "\n",
        "8. **Limitations:**\n",
        "   - GPT models might produce text that seems plausible but can be factually incorrect or exhibit biased behavior based on the training data.\n",
        "   - Handling specific user instructions or understanding context in a nuanced way can still be challenging.\n",
        "\n",
        "GPT models have shown remarkable capabilities in understanding and generating human-like text, but they are not infallible. Users should be aware of their limitations and carefully assess the output in critical applications."
      ],
      "metadata": {
        "id": "7EAfaDVv_2za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Code Explanation**"
      ],
      "metadata": {
        "id": "flRC8tdFn9DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Data Preprocessing (First Block):\n",
        "\n",
        "1. **Import Libraries:**\n",
        "   ```python\n",
        "   import os\n",
        "   import lzma\n",
        "   from tqdm import tqdm\n",
        "   ```\n",
        "\n",
        "   - The code imports necessary libraries for working with files and displaying progress bars.\n",
        "\n",
        "2. **Function to List \".xz\" Files in a Directory:**\n",
        "   ```python\n",
        "   def xz_files_in_dir(directory):\n",
        "       # ...\n",
        "   ```\n",
        "\n",
        "   - This function takes a directory path and returns a list of filenames with the \".xz\" extension.\n",
        "\n",
        "3. **Define Paths and Get File Lists:**\n",
        "   ```python\n",
        "   folder_path = \"/content/openwebtext\"\n",
        "   output_file_train = \"/content/output_train.txt\"\n",
        "   output_file_val = \"/content/output_val.txt\"\n",
        "   vocab_file = \"/content/vocab.txt\"\n",
        "\n",
        "   files = xz_files_in_dir(folder_path)\n",
        "   total_files = len(files)\n",
        "   ```\n",
        "\n",
        "   - Paths for input files, output files, and the vocabulary file are specified.\n",
        "   - The list of \".xz\" files in the given directory is obtained.\n",
        "\n",
        "4. **Calculate Split Indices and Process Files:**\n",
        "   ```python\n",
        "   split_index = int(total_files * 0.9)\n",
        "   files_train = files[:split_index]\n",
        "   files_val = files[split_index:]\n",
        "\n",
        "   vocab = set()\n",
        "\n",
        "   with open(output_file_train, \"w\", encoding=\"utf-8\") as outfile:\n",
        "       # ... (code for processing training files and updating vocabulary)\n",
        "\n",
        "   with open(output_file_val, \"w\", encoding=\"utf-8\") as outfile:\n",
        "       # ... (code for processing validation files and updating vocabulary)\n",
        "\n",
        "   with open(vocab_file, \"w\", encoding=\"utf-8\") as vfile:\n",
        "       # ... (code for writing vocabulary to file)\n",
        "   ```\n",
        "\n",
        "   - The code calculates the split index for training and validation files.\n",
        "   - It processes the training and validation files separately, updating the vocabulary.\n",
        "\n",
        "#### Model Definition (Second Block):\n",
        "\n",
        "1. **Import Libraries and Set Device:**\n",
        "   ```python\n",
        "   import torch\n",
        "   import torch.nn as nn\n",
        "   from torch.nn import functional as F\n",
        "   import mmap\n",
        "   import random\n",
        "   import pickle\n",
        "   import argparse\n",
        "   ```\n",
        "\n",
        "   - Necessary libraries for PyTorch, memory mapping, randomization, and argument parsing are imported.\n",
        "   - The device (CPU or GPU) is set based on availability.\n",
        "\n",
        "2. **Set Hyperparameters:**\n",
        "   ```python\n",
        "   batch_size = 32\n",
        "   block_size = 128\n",
        "   max_iters = 200\n",
        "   learning_rate = 3e-4\n",
        "   eval_iters = 100\n",
        "   n_embd = 384\n",
        "   n_head = 4\n",
        "   n_layer = 4\n",
        "   dropout = 0.2\n",
        "   ```\n",
        "\n",
        "   - Hyperparameters for the model training are set.\n",
        "\n",
        "4. **Get Random Chunk of Text:**\n",
        "   ```python\n",
        "   def get_random_chunk(split):\n",
        "       # ...\n",
        "   ```\n",
        "\n",
        "   - This function retrieves a random chunk of text from a specified file using memory mapping.\n",
        "\n",
        "5. **Function to Get Batch of Data:**\n",
        "   ```python\n",
        "   def get_batch(split):\n",
        "       # ...\n",
        "   ```\n",
        "\n",
        "   - This function obtains a batch of data for training or validation.\n"
      ],
      "metadata": {
        "id": "dgTe8OVgoFfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################################################################################################"
      ],
      "metadata": {
        "id": "H8CilNpxpB-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 6. Function to Estimate Loss (`estimate_loss`):\n",
        "\n",
        "```python\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "```\n",
        "\n",
        "- **Purpose:**\n",
        "  - This function is designed to estimate the average loss on both the training and validation sets over multiple iterations.\n",
        "\n",
        "- **Explanation:**\n",
        "  1. The function is defined with the `@torch.no_grad()` decorator to ensure that no gradients are calculated during the estimation.\n",
        "  2. The `model.eval()` method is called to set the model to evaluation mode, which disables dropout layers and other elements that behave differently during training.\n",
        "  3. A loop iterates over the training and validation sets.\n",
        "  4. For each set, another loop runs `eval_iters` times, sampling batches using the `get_batch` function.\n",
        "  5. The model is used to compute logits and loss for each batch, and the losses are accumulated.\n",
        "  6. The average loss for each set is computed and stored in the `out` dictionary.\n",
        "  7. The model is set back to training mode with `model.train()` before returning the results.\n",
        "\n",
        "#### 7. Classes for Attention Head, MultiHead Attention, FeedForward, and Transformer Block:\n",
        "\n",
        "- **Purpose:**\n",
        "  - These classes define the key components of a Transformer model, such as attention heads, multi-head attention, feedforward layers, and a transformer block.\n",
        "\n",
        "- **Explanation:**\n",
        "  1. **`Head` Class:**\n",
        "     - Represents a single head in the multi-head self-attention mechanism.\n",
        "     - Contains linear layers for key, query, and value projections.\n",
        "     - Applies dropout to the attention scores.\n",
        "\n",
        "  2. **`MultiHeadAttention` Class:**\n",
        "     - Combines multiple attention heads in parallel.\n",
        "     - Projects the concatenated output through a linear layer.\n",
        "     - Applies dropout to the aggregated output.\n",
        "\n",
        "  3. **`FeedForward` Class:**\n",
        "     - Implements a simple feedforward layer with ReLU activation.\n",
        "     - Contains two linear layers with dropout.\n",
        "\n",
        "  4. **`Block` Class:**\n",
        "     - Represents a transformer block, consisting of multi-head attention and feedforward layers.\n",
        "     - Applies layer normalization after each sub-block.\n",
        "\n",
        "#### 8. GPTLanguageModel Class:\n",
        "\n",
        "```python\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # ... (code for initializing model components)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        # ... (code for forward pass)\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # ... (code for text generation)\n",
        "```\n",
        "\n",
        "- **Purpose:**\n",
        "  - The `GPTLanguageModel` class defines the overall architecture of the GPT (Generative Pre-trained Transformer) language model.\n",
        "\n",
        "- **Explanation:**\n",
        "  1. **Initialization (`__init__` method):**\n",
        "     - Initializes the model by defining embedding layers, transformer blocks, layer normalization, and a linear layer for predicting the next token.\n",
        "\n",
        "  2. **Forward Pass (`forward` method):**\n",
        "     - Implements the forward pass of the model.\n",
        "     - Combines token and position embeddings, passes through transformer blocks, and predicts the next token.\n",
        "     - Computes and returns logits and loss if targets are provided.\n",
        "\n",
        "  3. **Text Generation (`generate` method):**\n",
        "     - Given an input index (context), generates new text by sampling tokens from the model.\n",
        "     - The sampling process is performed iteratively up to the specified maximum number of new tokens.\n",
        "\n",
        "#### 9. Initialization and Weight Initialization:\n",
        "\n",
        "```python\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "```\n",
        "\n",
        "- **Explanation:**\n",
        "  1. An instance of the `GPTLanguageModel` is created with the specified vocabulary size (`vocab_size`).\n",
        "  2. The model is then moved to the specified device (`device`), which can be either the GPU ('cuda') or CPU ('cpu').\n",
        "  3. Weight initialization is applied using the `self._init_weights` method, which initializes weights based on a normal distribution.\n",
        "\n",
        "This section concludes the model architecture, and the subsequent sections involve training the model, saving/loading the model, and generating text based on a prompt. If you have more specific questions or need clarification on any part, feel free to ask!"
      ],
      "metadata": {
        "id": "YJJMag9mnmw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################################################################################################"
      ],
      "metadata": {
        "id": "ygMjpYM4pHFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Model Training (Third Block):\n",
        "\n",
        "1. **Create Optimizer:**\n",
        "   ```python\n",
        "   optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "   ```\n",
        "\n",
        "   - An AdamW optimizer is created for training the model.\n",
        "\n",
        "2. **Training Loop:**\n",
        "   ```python\n",
        "   for iter in range(max_iters):\n",
        "       # ...\n",
        "   ```\n",
        "\n",
        "   - The training loop iterates through the specified number of epochs.\n",
        "\n",
        "3. **Sample Batch of Data, Evaluate Loss, and Backpropagate:**\n",
        "   ```python\n",
        "   xb, yb = get_batch('train')\n",
        "   logits, loss = model.forward(xb, yb)\n",
        "   optimizer.zero_grad(set_to_none=True)\n",
        "   loss.backward()\n",
        "   optimizer.step()\n",
        "   ```\n",
        "\n",
        "   - A batch of data is sampled, and the model's loss is computed and backpropagated.\n",
        "\n",
        "#### Model Generation (Fourth Block):\n",
        "\n",
        "1. **Save Trained Model:**\n",
        "   ```python\n",
        "   with open('model-01.pkl', 'wb') as f:\n",
        "       pickle.dump(model, f)\n",
        "   ```\n",
        "\n",
        "   - The trained model is saved to a file using pickle.\n",
        "\n",
        "2. **Generate Text Based on Prompt:**\n",
        "   ```python\n",
        "   prompt = 'Hello test test'\n",
        "   context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "   generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
        "   print(generated_chars)\n",
        "   ```\n",
        "\n",
        "   - An example prompt is given, encoded, and used to generate new text using the trained GPT model.\n",
        "   - The generated text is printed.\n",
        "\n",
        "Note: This breakdown covers the major steps and components of the code, but specific details of the transformer architecture and training process are within the model classes and functions. If you have specific questions about any part, feel free to ask!"
      ],
      "metadata": {
        "id": "NFx5AE4dowP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Extraxt Files**: from the open web corpus"
      ],
      "metadata": {
        "id": "SGFyqTtoEo-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import lzma\n",
        "from tqdm import tqdm\n",
        "\n",
        "def xz_files_in_dir(directory):\n",
        "    files = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".xz\") and os.path.isfile(os.path.join(directory, filename)):\n",
        "            files.append(filename)\n",
        "    return files\n",
        "\n",
        "folder_path = \"/content/openwebtext\"\n",
        "output_file_train = \"/content/output_train.txt\"\n",
        "output_file_val = \"/content/output_val.txt\"\n",
        "vocab_file = \"/content/vocab.txt\"\n",
        "\n",
        "files = xz_files_in_dir(folder_path)\n",
        "total_files = len(files)\n",
        "\n",
        "# Calculate the split indices\n",
        "split_index = int(total_files * 0.9) # 90% for training\n",
        "files_train = files[:split_index]\n",
        "files_val = files[split_index:]\n",
        "\n",
        "# Process the files for training and validation separately\n",
        "vocab = set()\n",
        "\n",
        "# Process the training files\n",
        "with open(output_file_train, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for filename in tqdm(files_train, total=len(files_train)):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
        "            text = infile.read()\n",
        "            outfile.write(text)\n",
        "            characters = set(text)\n",
        "            vocab.update(characters)\n",
        "\n",
        "# Process the validation files\n",
        "with open(output_file_val, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for filename in tqdm(files_val, total=len(files_val)):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
        "            text = infile.read()\n",
        "            outfile.write(text)\n",
        "            characters = set(text)\n",
        "            vocab.update(characters)\n",
        "\n",
        "# Write the vocabulary to vocab.txt\n",
        "with open(vocab_file, \"w\", encoding=\"utf-8\") as vfile:\n",
        "    for char in vocab:\n",
        "        vfile.write(char + '\\n')"
      ],
      "metadata": {
        "id": "akdDa7EnEtwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316be763-aab7-420c-aa7b-86fe554c821a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 144/144 [00:13<00:00, 11.06it/s]\n",
            "100%|██████████| 16/16 [00:01<00:00, 12.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Code**"
      ],
      "metadata": {
        "id": "6_lbolzY_7gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import mmap\n",
        "import random\n",
        "import pickle\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='This is a test')\n",
        "\n",
        "# Here we add an argument to the parser, specifying the expected type, a help message, etc.\n",
        "# parser.add_argument('-batch_size', type=str, required=True, help='Please provide a batch_size')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# Now we can use the argument value in our program.\n",
        "# print(f'batch size: {args.batch_size}')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# batch_size = args.batch_size # to use the batch_size cmd arg -> python file_name.py -batch_size 32\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters = 200\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 100\n",
        "n_embd = 384\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aoetbj6791Q",
        "outputId": "29becda3-e913-4eb7-9356-b861d299767d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = \"\"\n",
        "with open(\"/content/vocab.txt\", 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        chars = sorted(list(set(text)))\n",
        "\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "a4XSPft79BFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])"
      ],
      "metadata": {
        "id": "hVZUoZzp8_zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #memory map for using small snippets of text from a single file of any size\n",
        "def get_random_chunk(split):\n",
        "    filename = \"/content/output_train.txt\" if split == 'train' else \"/content/output_val.txt\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
        "            # Determine the file size and a random position to start reading\n",
        "            file_size = len(mm)\n",
        "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
        "\n",
        "            # Seek to the random position and read the block of text\n",
        "            mm.seek(start_pos)\n",
        "            block = mm.read(block_size*batch_size-1)\n",
        "\n",
        "            # Decode the block to a string, ignoring any invalid byte sequences\n",
        "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
        "\n",
        "            # Train and test splits\n",
        "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = get_random_chunk(split)\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "7XoXLsuS88hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out={}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses=torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X,Y=get_batch(split)\n",
        "      logits,loss=model(X,Y)\n",
        "      losses[k]=loss.item()\n",
        "    out[split]=losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "lUHJLnwh8D4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "C2LaifFi_Eg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "TD-MaIas-9w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "70ZYyZQ4-8zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.sa(x)\n",
        "        x = self.ln1(x + y)\n",
        "        y = self.ffwd(x)\n",
        "        x = self.ln2(x + y)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qPxHggDX-4Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        B, T = index.shape\n",
        "\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            index_cond = index[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self.forward(index_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "        return index\n",
        "\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "# print('loading model parameters...')\n",
        "# with open('model-01.pkl', 'rb') as f:\n",
        "#     model = pickle.load(f)\n",
        "# print('loaded successfully!')\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "hUtQl97H8t_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    print(iter)\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())\n",
        "\n",
        "with open('model-01.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "print('model saved')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4-LNdQ2_U5A",
        "outputId": "14745425-8dd2-4af5-b807-676ef32b2cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "step: 0, train loss: 8.478, val loss: 8.471\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "step: 100, train loss: 2.365, val loss: 2.386\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "2.333045244216919\n",
            "model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Hello test test'\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "id": "pW87qBZA_W4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9712abe5-7483-42b4-df59-5b65cc7eb6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello test testod oune.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " C iotat la YonoriourtheApil trches sectl’s the toutplhy meladedemofthery leD tentalay\n"
          ]
        }
      ]
    }
  ]
}