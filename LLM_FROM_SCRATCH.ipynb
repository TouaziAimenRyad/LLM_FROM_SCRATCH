{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "get a small data set to work on the bigram language model with\n",
        "\n",
        "it's a book with the licenceing pages and intro pages removed to not affect the prediction"
      ],
      "metadata": {
        "id": "f4zVzz80H-iM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading the Text File:**"
      ],
      "metadata": {
        "id": "NUYKQkNQF7yM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lRA-1EQFPLf",
        "outputId": "a18b39ff-3194-43e3-8bba-4ddd4c6e105d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-28 08:32:48--  https://github.com/Infatoshi/fcc-intro-to-llms/blob/main/wizard_of_oz.txt\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277641 (271K) [text/plain]\n",
            "Saving to: ‘wizard_of_oz.txt.1’\n",
            "\n",
            "wizard_of_oz.txt.1  100%[===================>] 271.13K  1.10MB/s    in 0.2s    \n",
            "\n",
            "2023-11-28 08:32:48 (1.10 MB/s) - ‘wizard_of_oz.txt.1’ saved [277641/277641]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/Infatoshi/fcc-intro-to-llms/blob/main/wizard_of_oz.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries:**"
      ],
      "metadata": {
        "id": "n7PV0gOEF_Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device ='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "eV8P60mtKzPn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading and Preprocessing the Text File:**"
      ],
      "metadata": {
        "id": "W-Q8GEtFGB07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('wizard_of_oz.txt','r',encoding='utf-8')as f :\n",
        "  text=f.read()\n",
        "\n",
        "chars=sorted(set(text))\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI8lCNqmGdCB",
        "outputId": "7afb0e37-93d4-4735-f6dd-a9d9c53c1f19"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '·', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size=len(chars)"
      ],
      "metadata": {
        "id": "aB3eiB8QdXrH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping Characters to Integers:**\n",
        "\n",
        "\n",
        "Creating mappings between characters and integers and vice versa. Also, defining functions to encode a string into a list of integers and decode a list of integers back to a string.\n"
      ],
      "metadata": {
        "id": "tAKsNXdhGF9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])"
      ],
      "metadata": {
        "id": "zPQxeEhbKLXC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Tensor from Encoded Text:**\n",
        "\n",
        "Converting the encoded text into a PyTorch tensor of type long.\n",
        "transforming our text corpus into a tensor data"
      ],
      "metadata": {
        "id": "310hYfGwK8m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofge7GAJKw7P",
        "outputId": "9324c8c7-83e1-44aa-cff4-9b7bb7491f97"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([83,  2, 72,  ..., 75,  2, 84])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "making tragets out of data for predictions using blocks method"
      ],
      "metadata": {
        "id": "5ifYYWBVOn5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size=8\n",
        "batch_size=4"
      ],
      "metadata": {
        "id": "C5cbChJHQEAU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Training and Validation Data:**\n",
        "\n"
      ],
      "metadata": {
        "id": "zF-W27_cLKry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=int(0.8*len(data))\n",
        "train_data=data[:n]\n",
        "val_data=data[n:]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOU4isgWLKGq",
        "outputId": "d61b51e7-84d3-4b74-c61b-6de68e1b61de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  2376,  40924, 117207, 134059])\n",
            "inputs :\n",
            " tensor([[70, 75,  0, 65, 70,  0, 74, 61],\n",
            "        [26, 54,  2,  0, 79, 64, 65, 75],\n",
            "        [76, 71, 68, 60,  0, 77, 75,  0],\n",
            "        [11,  0, 46, 64, 61, 70,  0, 64]], device='cuda:0') \n",
            " targets :\n",
            " tensor([[75,  0, 65, 70,  0, 74, 61],\n",
            "        [54,  2,  0, 79, 64, 65, 75],\n",
            "        [71, 68, 60,  0, 77, 75,  0],\n",
            "        [ 0, 46, 64, 61, 70,  0, 64]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Creation Function:**\n",
        "\n",
        "\n",
        "This function generates a batch of input and target sequences for training or validation.\n",
        "\n"
      ],
      "metadata": {
        "id": "ehViBNA8Gjsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_batch(split):\n",
        "  data=train_data if split=='train' else val_data\n",
        "  ix =torch.randint(len(data)-block_size,(batch_size,))\n",
        "  print(ix)\n",
        "  x=torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y=torch.stack([data[i+1:i+block_size] for i in ix])\n",
        "  x,y=x.to(device),y.to(device)\n",
        "  return x,y\n",
        "\n",
        "x,y = get_batch('train')\n",
        "print('inputs :\\n',x,'\\n','targets :\\n',y)"
      ],
      "metadata": {
        "id": "WnNAs1rNGjCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print Example Training Instances:**\n",
        "\n",
        "Printing example training instances with increasing context and corresponding targets.\n",
        "\n"
      ],
      "metadata": {
        "id": "I5LUe_avGqfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x=train_data[:block_size]\n",
        "y=train_data[1:block_size+1] #offesting by one\n",
        "\n",
        "for i in range(block_size):\n",
        "  context=x[:i+1]\n",
        "  target=y[i]\n",
        "  print('when input is ',context, ' the target is ',target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA50xsbZOnYY",
        "outputId": "da599d3e-dbf5-4a2b-e1db-759238e9d181"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is  tensor([83])  the target is  tensor(2)\n",
            "when input is  tensor([83,  2])  the target is  tensor(72)\n",
            "when input is  tensor([83,  2, 72])  the target is  tensor(57)\n",
            "when input is  tensor([83,  2, 72, 57])  the target is  tensor(81)\n",
            "when input is  tensor([83,  2, 72, 57, 81])  the target is  tensor(68)\n",
            "when input is  tensor([83,  2, 72, 57, 81, 68])  the target is  tensor(71)\n",
            "when input is  tensor([83,  2, 72, 57, 81, 68, 71])  the target is  tensor(57)\n",
            "when input is  tensor([83,  2, 72, 57, 81, 68, 71, 57])  the target is  tensor(60)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bigram Language Model Class:**\n",
        "\n",
        "The forward method takes an input index and optional targets. It calculates logits (unnormalized scores) using the token embedding table. If targets are provided, it calculates the cross-entropy loss between predicted logits and actual targets.\n",
        "\n",
        "\n",
        "The generate method generates new sequences given an initial context (index) and the maximum number of new tokens to generate. It repeatedly predicts the next token, samples from the distribution, and appends it to the sequence.\n",
        "\n",
        "Creating an instance of the BigramLanguageModel, moving it to the specified device, and initializing a context tensor for sequence generation.\n",
        "\n",
        "In summary, this code defines a simple bigram language model using PyTorch. It trains the model on a text dataset and showcases how to generate new sequences based on the learned patterns. The generated sequences reflect the model's understanding of the language structure based on the training data."
      ],
      "metadata": {
        "id": "k4GyckLPgFp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,index,targets=None):\n",
        "    logits=self.token_embedding_table(index)\n",
        "\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      B,T,C= logits.shape     #batch time channel\n",
        "      logits=logits.view(B*T,C)\n",
        "      targets=targets.view(B*T)\n",
        "      loss=F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits,loss\n",
        "\n",
        "  def generate(self,index,max_new_tokens):\n",
        "    #index is (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      #get the predictions\n",
        "      logits,loss= self.forward(index)\n",
        "      #focus only o the last time step\n",
        "      logits=logits[:,-1,:] #becomes (B,C)\n",
        "      #apply softmax to get probabilities\n",
        "      probs=F.softmax(logits,dim=-1)\n",
        "      #sample from distribution\n",
        "      index_next=torch.multinomial(probs,num_samples=1) #(B,1)\n",
        "      #append sampled index to runing sequence\n",
        "      index=torch.cat((index,index_next),dim=1)#(B,T+1)\n",
        "    return index\n",
        "\n",
        "model =BigramLanguageModel(vocabulary_size)\n",
        "m=model.to(device)\n",
        "context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
        "generated_chars=decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)\n"
      ],
      "metadata": {
        "id": "UF33jCV5gIo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1469e36d-734b-439f-bc59-1040ad270a16"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Yg0-﻿4v;{\\)IWE3)Kwj,bcc;﻿,*I{[]K&[s_YGBQA﻿ZU·i8*O_OXH%Ii﻿2m·EXMtI.A(eo%lPY9c2i[\\.sxfy4{mO'\\\\*8h5,S=;Q0iRcSAys'dPU}('Gj)qWKm) [:6ye·!UFyAv:Ue\"Y[7/WwuUSbzikJJN7lwCZP\"(\\gRxW·d W7dP3pHUynOL?!ql/X'n﻿hc1%,]F-nV1X,}p[73\"!0w3%PTq[YCcctVV1Vhi·)i·dP4ZKV,2*i*rWsxftNloCrWVb*\"A!v\\CXg_KMja8zX8\"RjLW8MXP8SI7SKJu43gDJRv_(\\bd]n%d'GBJz{IH,o3gXL;q·L:m!X8d8M:8﻿%*fd/ZkELN&J8QU!G1M:Ao!mk\";zE.VmP;1m&·?}F[v Y?SLM-84QWtj01\"JV1 PKeOLNl[8VbO{)M- 8p·i·QA3T/,mWW:SP4. El4,y%4Vr7;-a)(\"&,\"W-:9.0KcIgaJi5_pXi·CC=·(7NFjv8ZUF[7[9.]\n"
          ]
        }
      ]
    }
  ]
}